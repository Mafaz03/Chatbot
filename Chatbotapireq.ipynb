{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8x_XAIYFZ1Y"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('docs'):\n",
    "    os.makedirs('docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "SHCrT9ncFZyJ",
    "outputId": "1cc08760-13c2-477b-8c03-9d0347b2bc27"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-00223e26-7ecd-4a30-ba43-2948874eb294\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-00223e26-7ecd-4a30-ba43-2948874eb294\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving project-chatbot.docx to project-chatbot.docx\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "for filename in uploaded.keys():\n",
    "    os.rename(filename, 'docs/' + filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "615dhM62HVAj",
    "outputId": "3ea0c055-bdbe-441d-b758-5f8aedf4cb0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
      "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.5.5)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.42.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.1+cu121)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.5)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.8.2)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.112.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.30.5)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.18.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.26.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.19.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.64.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.2.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.12.3)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (30.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.5.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.6)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.27.0)\n",
      "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (24.1)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.37.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.5)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=8.0.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.26.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.26.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.47b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.47b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.47b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.47b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (71.0.4)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.20.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.6.20)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (13.7.1)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.0.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.19.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.16.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fdp5IRQZHqza",
    "outputId": "4eae111b-98b3-42bf-800f-09fdc63acca6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Team 1 \n",
      "\n",
      "Mohamed Faraazman Bin Farooq S | Linga vara Prasad\n",
      "\n",
      "Task name LSTM\n",
      "\n",
      "\n",
      "\n",
      "# Preprocess Your Data\n",
      "\n",
      "from tensorflow.keras.preprocessing.text import Tokenizer\n",
      "\n",
      "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
      "\n",
      "from tensorflow.keras.utils import to_categorical\n",
      "\n",
      "\n",
      "\n",
      "# Sample data\n",
      "\n",
      "input_texts = [\"How much is this product?\", \"Where is the dairy section?\"]\n",
      "\n",
      "target_texts = [\"This product costs $5.\", \"The dairy section is in aisle 3.\"]\n",
      "\n",
      "\n",
      "\n",
      "# Tokenize\n",
      "\n",
      "tokenizer_input = Tokenizer()\n",
      "\n",
      "tokenizer_input.fit_on_texts(input_texts)\n",
      "\n",
      "input_sequences = tokenizer_input.texts_to_sequences(input_texts)\n",
      "\n",
      "\n",
      "\n",
      "tokenizer_target = Tokenizer()\n",
      "\n",
      "tokenizer_target.fit_on_texts(target_texts)\n",
      "\n",
      "target_sequences = tokenizer_target.texts_to_sequences(target_texts)\n",
      "\n",
      "\n",
      "\n",
      "# Convert to one-hot encoded vectors\n",
      "\n",
      "max_encoder_seq_length = max(len(seq) for seq in input_sequences)\n",
      "\n",
      "max_decoder_seq_length = max(len(seq) for seq in target_sequences)\n",
      "\n",
      "num_encoder_tokens = len(tokenizer_input.word_index) + 1\n",
      "\n",
      "num_decoder_tokens = len(tokenizer_target.word_index) + 1\n",
      "\n",
      "\n",
      "\n",
      "encoder_input_data = pad_sequences(input_sequences, maxlen=max_encoder_seq_length)\n",
      "\n",
      "decoder_input_data = pad_sequences(target_sequences, maxlen=max_decoder_seq_length)\n",
      "\n",
      "decoder_target_data = np.zeros((len(target_sequences), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
      "\n",
      "\n",
      "\n",
      "for i, seq in enumerate(target_sequences):\n",
      "\n",
      "    for t, token in enumerate(seq):\n",
      "\n",
      "        if t > 0:\n",
      "\n",
      "            decoder_target_data[i, t - 1, token] = 1.0\n",
      "\n",
      "# Build the Seq2Seq Model   \n",
      "\n",
      "from tensorflow.keras.models import Model\n",
      "\n",
      "from tensorflow.keras.layers import Input, LSTM, Dense\n",
      "\n",
      "\n",
      "\n",
      "# Encoder\n",
      "\n",
      "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
      "\n",
      "encoder = LSTM(256, return_state=True)\n",
      "\n",
      "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
      "\n",
      "encoder_states = [state_h, state_c]\n",
      "\n",
      "\n",
      "\n",
      "# Decoder\n",
      "\n",
      "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
      "\n",
      "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
      "\n",
      "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
      "\n",
      "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
      "\n",
      "decoder_outputs = decoder_dense(decoder_outputs)\n",
      "\n",
      "\n",
      "\n",
      "# Model\n",
      "\n",
      "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
      "\n",
      "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
      "\n",
      "\n",
      "\n",
      "# Train the model\n",
      "\n",
      "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=64, epochs=100, validation_split=0.2)\n",
      "\n",
      "# Define Inference Models\n",
      "\n",
      "# Encoder model for inference\n",
      "\n",
      "encoder_model = Model(encoder_inputs, encoder_states)\n",
      "\n",
      "\n",
      "\n",
      "# Decoder model for inference\n",
      "\n",
      "decoder_state_input_h = Input(shape=(256,))\n",
      "\n",
      "decoder_state_input_c = Input(shape=(256,))\n",
      "\n",
      "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
      "\n",
      "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
      "\n",
      "decoder_states = [state_h, state_c]\n",
      "\n",
      "decoder_outputs = decoder_dense(decoder_outputs)\n",
      "\n",
      "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
      "\n",
      "# Create a Response Function\n",
      "\n",
      "def decode_sequence(input_seq):\n",
      "\n",
      "    states_value = encoder_model.predict(input_seq)\n",
      "\n",
      "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
      "\n",
      "    target_seq[0, 0, tokenizer_target.word_index['\\t']] = 1.0  # Start token\n",
      "\n",
      "\n",
      "\n",
      "    stop_condition = False\n",
      "\n",
      "    decoded_sentence = ''\n",
      "\n",
      "    while not stop_condition:\n",
      "\n",
      "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
      "\n",
      "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
      "\n",
      "        sampled_char = tokenizer_target.index_word[sampled_token_index]\n",
      "\n",
      "\n",
      "\n",
      "        if sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length:\n",
      "\n",
      "            stop_condition = True\n",
      "\n",
      "\n",
      "\n",
      "        decoded_sentence += ' ' + sampled_char\n",
      "\n",
      "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
      "\n",
      "        target_seq[0, 0, sampled_token_index] = 1.0\n",
      "\n",
      "        states_value = [h, c]\n",
      "\n",
      "\n",
      "\n",
      "    return decoded_sentence\n",
      "\n",
      "# Test the Model\n",
      "\n",
      "for seq_index in range(len(input_texts)):\n",
      "\n",
      "    input_seq = pad_sequences([input_sequences[seq_index]], maxlen=max_encoder_seq_length)\n",
      "\n",
      "    decoded_sentence = decode_sequence(input_seq)\n",
      "\n",
      "    print(f'Input: {input_texts[seq_index]}')\n",
      "\n",
      "    print(f'Decoded: {decoded_sentence}')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Team 2\n",
      "\n",
      "Ankit kumar | \n",
      "\n",
      "Task name transformer\n",
      "\n",
      "\n",
      "\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.keras.layers import Input, Dense, Embedding, Dropout, LayerNormalization, MultiHeadAttention\n",
      "\n",
      "from tensorflow.keras.models import Model\n",
      "\n",
      "\n",
      "\n",
      "# Function to define a Transformer Encoder layer\n",
      "\n",
      "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    Applies the Transformer Encoder layer which includes multi-head self-attention\n",
      "\n",
      "    and feed-forward neural network.\n",
      "\n",
      "\n",
      "\n",
      "    Args:\n",
      "\n",
      "    - inputs: Input tensor to the encoder layer.\n",
      "\n",
      "    - head_size: Dimensionality of the attention heads.\n",
      "\n",
      "    - num_heads: Number of attention heads.\n",
      "\n",
      "    - ff_dim: Dimensionality of the feed-forward layer.\n",
      "\n",
      "    - dropout: Dropout rate for regularization.\n",
      "\n",
      "\n",
      "\n",
      "    Returns:\n",
      "\n",
      "    - Output tensor after applying the encoder layer.\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    # Layer Normalization and Self-Attention\n",
      "\n",
      "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
      "\n",
      "    x = MultiHeadAttention(\n",
      "\n",
      "        key_dim=head_size, \n",
      "\n",
      "        num_heads=num_heads, \n",
      "\n",
      "        dropout=dropout\n",
      "\n",
      "    )(x, x)\n",
      "\n",
      "    x = Dropout(dropout)(x)\n",
      "\n",
      "    # Residual Connection\n",
      "\n",
      "    res = x + inputs\n",
      "\n",
      "\n",
      "\n",
      "    # Feed Forward Network\n",
      "\n",
      "    x = LayerNormalization(epsilon=1e-6)(res)\n",
      "\n",
      "    x = Dense(ff_dim, activation=\"relu\")(x)\n",
      "\n",
      "    x = Dropout(dropout)(x)\n",
      "\n",
      "    x = Dense(inputs.shape[-1])(x)\n",
      "\n",
      "    return x + res\n",
      "\n",
      "\n",
      "\n",
      "# Function to define a Transformer Decoder layer\n",
      "\n",
      "def transformer_decoder(target, context, head_size, num_heads, ff_dim, dropout=0):\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    Applies the Transformer Decoder layer which includes multi-head self-attention,\n",
      "\n",
      "    cross-attention with encoder outputs, and feed-forward neural network.\n",
      "\n",
      "\n",
      "\n",
      "    Args:\n",
      "\n",
      "    - target: Input tensor to the decoder layer (usually the target sequence).\n",
      "\n",
      "    - context: Encoder output that provides context for decoding.\n",
      "\n",
      "    - head_size: Dimensionality of the attention heads.\n",
      "\n",
      "    - num_heads: Number of attention heads.\n",
      "\n",
      "    - ff_dim: Dimensionality of the feed-forward layer.\n",
      "\n",
      "    - dropout: Dropout rate for regularization.\n",
      "\n",
      "\n",
      "\n",
      "    Returns:\n",
      "\n",
      "    - Output tensor after applying the decoder layer.\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    # Layer Normalization and Self-Attention\n",
      "\n",
      "    x = LayerNormalization(epsilon=1e-6)(target)\n",
      "\n",
      "    x = MultiHeadAttention(\n",
      "\n",
      "        key_dim=head_size, \n",
      "\n",
      "        num_heads=num_heads, \n",
      "\n",
      "        dropout=dropout\n",
      "\n",
      "    )(x, x)\n",
      "\n",
      "    x = Dropout(dropout)(x)\n",
      "\n",
      "    x = x + target\n",
      "\n",
      "\n",
      "\n",
      "    # Layer Normalization and Cross-Attention with Encoder Output\n",
      "\n",
      "    x = LayerNormalization(epsilon=1e-6)(x)\n",
      "\n",
      "    x = MultiHeadAttention(\n",
      "\n",
      "        key_dim=head_size, \n",
      "\n",
      "        num_heads=num_heads, \n",
      "\n",
      "        dropout=dropout\n",
      "\n",
      "    )(x, context)\n",
      "\n",
      "    x = Dropout(dropout)(x)\n",
      "\n",
      "    x = x + target\n",
      "\n",
      "\n",
      "\n",
      "    # Feed Forward Network\n",
      "\n",
      "    x = LayerNormalization(epsilon=1e-6)(x)\n",
      "\n",
      "    x = Dense(ff_dim, activation=\"relu\")(x)\n",
      "\n",
      "    x = Dropout(dropout)(x)\n",
      "\n",
      "    x = Dense(target.shape[-1])(x)\n",
      "\n",
      "    return x + target\n",
      "\n",
      "\n",
      "\n",
      "# Function to build the full Transformer model\n",
      "\n",
      "def build_transformer(vocab_size, max_len, num_layers, head_size, num_heads, ff_dim, dropout=0):\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    Builds the complete Transformer model with both encoder and decoder.\n",
      "\n",
      "\n",
      "\n",
      "    Args:\n",
      "\n",
      "    - vocab_size: Size of the vocabulary (number of unique tokens).\n",
      "\n",
      "    - max_len: Maximum length of input and output sequences.\n",
      "\n",
      "    - num_layers: Number of Transformer layers in both encoder and decoder.\n",
      "\n",
      "    - head_size: Dimensionality of the attention heads.\n",
      "\n",
      "    - num_heads: Number of attention heads.\n",
      "\n",
      "    - ff_dim: Dimensionality of the feed-forward layers.\n",
      "\n",
      "    - dropout: Dropout rate for regularization.\n",
      "\n",
      "\n",
      "\n",
      "    Returns:\n",
      "\n",
      "    - The Transformer model instance.\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    # Input for encoder\n",
      "\n",
      "    inputs = Input(shape=(max_len,))\n",
      "\n",
      "    x = Embedding(vocab_size, head_size)(inputs)\n",
      "\n",
      "    for _ in range(num_layers):\n",
      "\n",
      "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
      "\n",
      "    \n",
      "\n",
      "    # Input for decoder\n",
      "\n",
      "    targets = Input(shape=(max_len,))\n",
      "\n",
      "    y = Embedding(vocab_size, head_size)(targets)\n",
      "\n",
      "    for _ in range(num_layers):\n",
      "\n",
      "        y = transformer_decoder(y, x, head_size, num_heads, ff_dim, dropout)\n",
      "\n",
      "    \n",
      "\n",
      "    # Output layer\n",
      "\n",
      "    outputs = Dense(vocab_size, activation='softmax')(y)\n",
      "\n",
      "\n",
      "\n",
      "    # Create the model\n",
      "\n",
      "    model = Model(inputs=[inputs, targets], outputs=outputs)\n",
      "\n",
      "    return model\n",
      "\n",
      "\n",
      "\n",
      "# Define model parameters\n",
      "\n",
      "vocab_size = 10000  # Size of vocabulary\n",
      "\n",
      "max_len = 50        # Maximum length of input and output sequences\n",
      "\n",
      "num_layers = 4      # Number of Transformer layers\n",
      "\n",
      "head_size = 64      # Dimensionality of the attention heads\n",
      "\n",
      "num_heads = 4       # Number of attention heads\n",
      "\n",
      "ff_dim = 128        # Dimensionality of the feed-forward layers\n",
      "\n",
      "dropout = 0.1       # Dropout rate\n",
      "\n",
      "\n",
      "\n",
      "# Build the model\n",
      "\n",
      "model = build_transformer(vocab_size, max_len, num_layers, head_size, num_heads, ff_dim, dropout)\n",
      "\n",
      "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
      "\n",
      "\n",
      "\n",
      "# Print model summary\n",
      "\n",
      "model.summary()\n",
      "\n",
      "\n",
      "\n",
      "# Example dummy data for training\n",
      "\n",
      "X_train = [np.random.randint(0, vocab_size, (1000, max_len)), np.random.randint(0, vocab_size, (1000, max_len))]\n",
      "\n",
      "y_train = np.random.randint(0, vocab_size, (1000, max_len))\n",
      "\n",
      "\n",
      "\n",
      "# Train the model\n",
      "\n",
      "model.fit(X_train, y_train, epochs=1, batch_size=32)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset\n",
      "\n",
      "conversation_id,turn_id,sender,message,timestamp\n",
      "\n",
      "1,1,user,Hello,2024-07-16 10:00:00\n",
      "\n",
      "1,2,bot,Hi! How can I help you today?,2024-07-16 10:00:02\n",
      "\n",
      "1,3,user,Can you tell me if you have organic apples in stock?,2024-07-16 10:00:05\n",
      "\n",
      "1,4,bot,Yes, we have organic apples in stock. Anything else I can help you with?,2024-07-16 10:00:07\n",
      "\n",
      "2,1,user,What are your store hours?,2024-07-16 11:00:00\n",
      "\n",
      "2,2,bot,We are open from 8 AM to 10 PM every day.,2024-07-16 11:00:02\n",
      "\n",
      "3,1,user,Do you have any discounts on dairy products?,2024-07-16 12:00:00\n",
      "\n",
      "3,2,bot,Yes, we currently have a 10% discount on all dairy products.,2024-07-16 12:00:02\n",
      "\n",
      "4,1,user,Can you help me find gluten-free bread?,2024-07-16 13:00:00\n",
      "\n",
      "4,2,bot,Sure, we have gluten-free bread in aisle 5.,2024-07-16 13:00:02\n",
      "\n",
      "5,1,user,What is the price of your organic milk?,2024-07-16 14:00:00\n",
      "\n",
      "5,2,bot,Organic milk is priced at $3.99 per liter.,2024-07-16 14:00:02\n",
      "\n",
      "6,1,user,Do you offer home delivery?,2024-07-16 15:00:00\n",
      "\n",
      "6,2,bot,Yes, we offer home delivery for orders over $50.,2024-07-16 15:00:02\n",
      "\n",
      "7,1,user,How can I return a product?,2024-07-16 16:00:00\n",
      "\n",
      "7,2,bot,You can return a product within 30 days of purchase with a receipt.,2024-07-16 16:00:02\n",
      "\n",
      "8,1,user,Are there any vegan options available?,2024-07-16 17:00:00\n",
      "\n",
      "8,2,bot,Yes, we have a variety of vegan options available in the store.,2024-07-16 17:00:02\n",
      "\n",
      "9,1,user,Can I pay with digital wallets?,2024-07-16 18:00:00\n",
      "\n",
      "9,2,bot,Yes, we accept payments via digital wallets such as Apple Pay and Google Pay.,2024-07-16 18:00:02\n",
      "\n",
      "10,1,user,Where can I find the bakery section?,2024-07-16 19:00:00\n",
      "\n",
      "10,2,bot,The bakery section is located near the entrance of the store.,2024-07-16 19:00:02\n",
      "\n",
      "11,1,user,Do you have fresh fish available?,2024-07-16 20:00:00\n",
      "\n",
      "11,2,bot,Yes, we have a variety of fresh fish available daily.,2024-07-16 20:00:02\n",
      "\n",
      "12,1,user,What is your return policy on perishables?,2024-07-16 21:00:00\n",
      "\n",
      "12,2,bot,Perishables can be returned within 7 days of purchase with a receipt.,2024-07-16 21:00:02\n",
      "\n",
      "13,1,user,Can I order groceries online?,2024-07-16 22:00:00\n",
      "\n",
      "13,2,bot,Yes, you can place orders on our website and choose home delivery or store pickup.,2024-07-16 22:00:02\n",
      "\n",
      "14,1,user,Do you have a loyalty program?,2024-07-17 09:00:00\n",
      "\n",
      "14,2,bot,Yes, we have a loyalty program where you can earn points for every purchase.,2024-07-17 09:00:02\n",
      "\n",
      "15,1,user,Can I get assistance with carrying my groceries to my car?,2024-07-17 10:00:00\n",
      "\n",
      "15,2,bot,Yes, our staff can assist you with carrying groceries to your car.,2024-07-17 10:00:02\n",
      "\n",
      "16,1,user,What are the benefits of your loyalty program?,2024-07-17 11:00:00\n",
      "\n",
      "16,2,bot,You can earn points on purchases, get exclusive discounts, and receive special offers.,2024-07-17 11:00:02\n",
      "\n",
      "17,1,user,Do you sell gift cards?,2024-07-17 12:00:00\n",
      "\n",
      "17,2,bot,Yes, we sell gift cards in various denominations.,2024-07-17 12:00:02\n",
      "\n",
      "18,1,user,Is there a pharmacy in your store?,2024-07-17 13:00:00\n",
      "\n",
      "18,2,bot,Yes, we have a pharmacy in our store.,2024-07-17 13:00:02\n",
      "\n",
      "19,1,user,Do you offer any cooking classes?,2024-07-17 14:00:00\n",
      "\n",
      "19,2,bot,Yes, we offer cooking classes. You can check our schedule on the website.,2024-07-17 14:00:02\n",
      "\n",
      "20,1,user,Can I place a bulk order for a party?,2024-07-17 15:00:00\n",
      "\n",
      "20,2,bot,Yes, you can place bulk orders by contacting our customer service.,2024-07-17 15:00:02\n",
      "\n",
      "21,1,user,Do you have a section for international foods?,2024-07-17 16:00:00\n",
      "\n",
      "21,2,bot,Yes, we have an aisle dedicated to international foods.,2024-07-17 16:00:02\n",
      "\n",
      "22,1,user,Can I get a refund if the product is expired?,2024-07-17 17:00:00\n",
      "\n",
      "22,2,bot,Yes, you can get a refund or exchange if the product is expired.,2024-07-17 17:00:02\n",
      "\n",
      "23,1,user,Do you have organic vegetables?,2024-07-17 18:00:00\n",
      "\n",
      "23,2,bot,Yes, we have a wide range of organic vegetables available.,2024-07-17 18:00:02\n",
      "\n",
      "24,1,user,Can I schedule a delivery for a specific time?,2024-07-17 19:00:00\n",
      "\n",
      "24,2,bot,Yes, you can choose a delivery slot that fits your schedule.,2024-07-17 19:00:02\n",
      "\n",
      "25,1,user,What forms of payment do you accept?,2024-07-17 20:00:00\n",
      "\n",
      "25,2,bot,We accept cash, credit/debit cards, and digital wallets.,2024-07-17 20:00:02\n",
      "\n",
      "26,1,user,Do you have a bakery?,2024-07-17 21:00:00\n",
      "\n",
      "26,2,bot,Yes, we have a bakery that offers fresh bread and pastries.,2024-07-17 21:00:02\n",
      "\n",
      "27,1,user,What is the price of avocados?,2024-07-17 22:00:00\n",
      "\n",
      "27,2,bot,Avocados are priced at $1.50 each.,2024-07-17 22:00:02\n",
      "\n",
      "28,1,user,Do you sell non-dairy milk?,2024-07-18 09:00:00\n",
      "\n",
      "28,2,bot,Yes, we have a variety of non-dairy milk options including almond, soy, and oat milk.,2024-07-18 09:00:02\n",
      "\n",
      "29,1,user,Where can I find cleaning supplies?,2024-07-18 10:00:00\n",
      "\n",
      "29,2,bot,Cleaning supplies are located in aisle 7.,2024-07-18 10:00:02\n",
      "\n",
      "30,1,user,Do you have a customer service desk?,2024-07-18 11:00:00\n",
      "\n",
      "30,2,bot,Yes, our customer service desk is located near the store entrance.,2024-07-18 11:00:02\n",
      "\n",
      "31,1,user,Can I buy alcohol in your store?,2024-07-18 12:00:00\n",
      "\n",
      "31,2,bot,Yes, we have a wide selection of alcoholic beverages.,2024-07-18 12:00:02\n",
      "\n",
      "32,1,user,What is your policy on price matching?,2024-07-18 13:00:00\n",
      "\n",
      "32,2,bot,We offer price matching on identical items with proof of a lower price.,2024-07-18 13:00:02\n",
      "\n",
      "33,1,user,Do you sell pet food?,2024-07-18 14:00:00\n",
      "\n",
      "33,2,bot,Yes, we have a variety of pet food and supplies.,2024-07-18 14:00:02\n",
      "\n",
      "34,1,user,Can I get a rain check for a sale item?,2024-07-18 15:00:00\n",
      "\n",
      "34,2,bot,Yes, we offer rain checks for out-of-stock sale items.,2024-07-18 15:00:02\n",
      "\n",
      "35,1,user,Do you offer senior discounts?,2024-07-18 16:00:00\n",
      "\n",
      "35,2,bot,Yes, we offer a 5% discount for seniors on Tuesdays.,2024-07-18 16:00:02\n",
      "\n",
      "36,1,user,Are there any gluten-free pasta options?,2024-07-18 17:00:00\n",
      "\n",
      "36,2,bot,Yes, we have several gluten-free pasta options.,2024-07-18 17:00:02\n",
      "\n",
      "37,1,user,What is the price of bananas?,2024-07-18 18:00:00\n",
      "\n",
      "37,2,bot,Bananas are priced at $0.99 per pound.,2024-07-18 18:00:02\n",
      "\n",
      "38,1,user,Can I buy bulk spices?,2024-07-18 19:00:00\n",
      "\n",
      "38,2,bot,Yes, we offer a variety of spices in bulk.,2024-07-18 19:00:02\n",
      "\n",
      "39,1,user,Do you have a florist section?,2024-07-18 20:00:00\n",
      "\n",
      "39,2,bot,Yes, we have a florist section with fresh flowers and arrangements.,2024-07-18 20:00:02\n",
      "\n",
      "40,1,user,What is the price of a gallon of water?,2024-07-18 21:00:00\n",
      "\n",
      "40,2,bot,A gallon of water is priced at $0.89.,2024-07-18 21:00:02\n",
      "\n",
      "41,1,user,Do you have a butcher?,2024-07-18 22:00:00\n",
      "\n",
      "41,2,bot,Yes, we have a butcher who can assist with custom cuts of meat.,2024-07-18 22:00:02\n",
      "\n",
      "42,1,user,Can I recycle plastic bags in your store?,2024-07-19 09:00:00\n",
      "\n",
      "42,2,bot,Yes, we have a recycling bin for plastic bags near the entrance.,2024-07-19 09:00:02\n",
      "\n",
      "43,1,user,Do you have self-checkout kiosks?,2024-07-19 10:00:00\n",
      "\n",
      "43,2,bot,Yes, we have self-checkout kiosks for your convenience.,2024-07-19 10:00:02\n",
      "\n",
      "44,1,user,Can I get a custom cake made?,2024-07-19 11:00:00\n",
      "\n",
      "44,2,bot,Yes, our bakery can make custom cakes to order.,2024-07-19 11:00:02\n",
      "\n",
      "45,1,user,Do you sell batteries?,2024-07-19 12:00:00\n",
      "\n",
      "45,2,bot,Yes, we sell a variety of batteries.,2024-07-19 12:00:02\n",
      "\n",
      "46,1,user,Can I order a deli platter?,2024-07-19 13:00:00\n",
      "\n",
      "46,2,bot,Yes, you can order a deli platter by contacting our deli department.,2024-07-19 13:00:02\n",
      "\n",
      "47,1,user,What is the price of ground beef?,2024-07-19 14:00:00\n",
      "\n",
      "47,2,bot,Ground beef is priced at $4.99 per pound.,2024-07-19 14:00:02\n",
      "\n",
      "48,1,user,Do you have a section for bulk items?,2024-07-19 15:00:00\n",
      "\n",
      "48,2,bot,Yes, we have a section dedicated to bulk items.,2024-07-19 15:00:02\n",
      "\n",
      "49,1,user,What is the price of eggs?,2024-07-19 16:00:00\n",
      "\n",
      "49,2,bot,A dozen eggs are priced at $2.99.,2024-07-19 16:00:02\n",
      "\n",
      "50,1,user,Do you sell baby products?,2024-07-19 17:00:00\n",
      "\n",
      "50,2,bot,Yes, we have a range of baby products available.,2024-07-19 17:00:02\n",
      "\n",
      "51,1,user,Can I get a refund for a damaged item?,2024-07-19 18:00:00\n",
      "\n",
      "51,2,bot,Yes, you can get a refund or exchange for a damaged item with a receipt.,2024-07-19 18:00:02\n",
      "\n",
      "52,1,user,Do you offer curbside pickup?,2024-07-19 19:00:00\n",
      "\n",
      "52,2,bot,Yes, we offer curbside pickup for online orders.,2024-07-19 19:00:02\n",
      "\n",
      "53,1,user,What is the price of your deli sandwiches?,2024-07-19 20:00:00\n",
      "\n",
      "53,2,bot,Deli sandwiches are priced between $5.99 and $8.99 depending on the type.,2024-07-19 20:00:02\n",
      "\n",
      "54,1,user,Can I buy prepared meals?,2024-07-19 21:00:00\n",
      "\n",
      "54,2,bot,Yes, we have a selection of prepared meals in our deli section.,2024-07-19 21:00:02\n",
      "\n",
      "55,1,user,Do you have a cafe?,2024-07-19 22:00:00\n",
      "\n",
      "55,2,bot,Yes, we have a cafe that serves coffee, tea, and light snacks.,2024-07-19 22:00:02\n",
      "\n",
      "56,1,user,Can I return a product without a receipt?,2024-07-20 09:00:00\n",
      "\n",
      "56,2,bot,Returns without a receipt are subject to store policy and may only be exchanged for store credit.,2024-07-20 09:00:02\n",
      "\n",
      "57,1,user,Do you offer catering services?,2024-07-20 10:00:00\n",
      "\n",
      "57,2,bot,Yes, we offer catering services. Please contact customer service for more details.,2024-07-20 10:00:02\n",
      "\n",
      "58,1,user,Can I find kosher products in your store?,2024-07-20 11:00:00\n",
      "\n",
      "58,2,bot,Yes, we have a selection of kosher products.,2024-07-20 11:00:02\n",
      "\n",
      "59,1,user,Do you sell lottery tickets?,2024-07-20 12:00:00\n",
      "\n",
      "59,2,bot,Yes, we sell lottery tickets at the customer service desk.,2024-07-20 12:00:02\n",
      "\n",
      "60,1,user,Can I get help with a large order?,2024-07-20 13:00:00\n",
      "\n",
      "60,2,bot,Yes, our staff can assist you with large orders. Please let us know in advance.,2024-07-20 13:00:02\n",
      "\n",
      "61,1,user,What is the price of a rotisserie chicken?,2024-07-20 14:00:00\n",
      "\n",
      "61,2,bot,A rotisserie chicken is priced at $7.99.,2024-07-20 14:00:02\n",
      "\n",
      "62,1,user,Do you sell fresh herbs?,2024-07-20 15:00:00\n",
      "\n",
      "62,2,bot,Yes, we have a variety of fresh herbs available.,2024-07-20 15:00:02\n",
      "\n",
      "63,1,user,Can I use coupons at self-checkout?,2024-07-20 16:00:00\n",
      "\n",
      "63,2,bot,Yes, you can use coupons at self-checkout kiosks.,2024-07-20 16:00:02\n",
      "\n",
      "64,1,user,What is the price of strawberries?,2024-07-20 17:00:00\n",
      "\n",
      "64,2,bot,Strawberries are priced at $3.99 per pound.,2024-07-20 17:00:02\n",
      "\n",
      "65,1,user,Do you sell alcohol-free wine?,2024-07-20 18:00:00\n",
      "\n",
      "65,2,bot,Yes, we have alcohol-free wine options.,2024-07-20 18:00:02\n",
      "\n",
      "66,1,user,Can I get assistance with finding products?,2024-07-20 19:00:00\n",
      "\n",
      "66,2,bot,Yes, our staff can help you find products in the store.,2024-07-20 19:00:02\n",
      "\n",
      "67,1,user,Do you have a lost and found?,2024-07-20 20:00:00\n",
      "\n",
      "67,2,bot,Yes, our lost and found is located at the customer service desk.,2024-07-20 20:00:02\n",
      "\n",
      "68,1,user,What is the price of a whole turkey?,2024-07-20 21:00:00\n",
      "\n",
      "68,2,bot,A whole turkey is priced at $1.99 per pound.,2024-07-20 21:00:02\n",
      "\n",
      "69,1,user,Do you sell magazines?,2024-07-20 22:00:00\n",
      "\n",
      "69,2,bot,Yes, we have a variety of magazines available.,2024-07-20 22:00:02\n",
      "\n",
      "70,1,user,Can I get a discount for buying in bulk?,2024-07-21 09:00:00\n",
      "\n",
      "70,2,bot,Yes, we offer discounts on bulk purchases. Please check with our customer service for details.,2024-07-21 09:00:02\n",
      "\n",
      "71,1,user,Do you sell frozen meals?,2024-07-21 10:00:00\n",
      "\n",
      "71,2,bot,Yes, we have a wide range of frozen meals available.,2024-07-21 10:00:02\n",
      "\n",
      "72,1,user,Can I return a gift card?,2024-07-21 11:00:00\n",
      "\n",
      "72,2,bot,Gift cards are non-refundable.,2024-07-21 11:00:02\n",
      "\n",
      "73,1,user,Do you offer discounts for students?,2024-07-21 12:00:00\n",
      "\n",
      "73,2,bot,Yes, we offer a 5% discount for students with a valid student ID.,2024-07-21 12:00:02\n",
      "\n",
      "74,1,user,Can I buy fresh seafood?,2024-07-21 13:00:00\n",
      "\n",
      "74,2,bot,Yes, we have a fresh seafood section.,2024-07-21 13:00:02\n",
      "\n",
      "75,1,user,What is the price of oranges?,2024-07-21 14:00:00\n",
      "\n",
      "75,2,bot,Oranges are priced at $1.20 per pound.,2024-07-21 14:00:02\n",
      "\n",
      "76,1,user,Do you have a bakery that makes custom bread?,2024-07-21 15:00:00\n",
      "\n",
      "76,2,bot,Yes, our bakery can make custom bread to your specifications.,2024-07-21 15:00:02\n",
      "\n",
      "77,1,user,Can I buy a subscription to a meal kit service?,2024-07-21 16:00:00\n",
      "\n",
      "77,2,bot,Yes, we offer meal kit subscriptions. Please check our website for details.,2024-07-21 16:00:02\n",
      "\n",
      "78,1,user,Do you sell ice cream cakes?,2024-07-21 17:00:00\n",
      "\n",
      "78,2,bot,Yes, we have a selection of ice cream cakes available.,2024-07-21 17:00:02\n",
      "\n",
      "79,1,user,What is the price of blueberries?,2024-07-21 18:00:00\n",
      "\n",
      "79,2,bot,Blueberries are priced at $4.50 per pound.,2024-07-21 18:00:02\n",
      "\n",
      "80,1,user,Do you offer free samples?,2024-07-21 19:00:00\n",
      "\n",
      "80,2,bot,Yes, we offer free samples in various sections of the store.,2024-07-21 19:00:02\n",
      "\n",
      "81,1,user,Can I find organic snacks?,2024-07-21 20:00:00\n",
      "\n",
      "81,2,bot,Yes, we have a variety of organic snacks.,2024-07-21 20:00:02\n",
      "\n",
      "82,1,user,Do you sell household appliances?,2024-07-21 21:00:00\n",
      "\n",
      "82,2,bot,Yes, we have a selection of small household appliances.,2024-07-21 21:00:02\n",
      "\n",
      "83,1,user,What is the price of your rotisserie chicken?,2024-07-21 22:00:00\n",
      "\n",
      "83,2,bot,Our rotisserie chicken is priced at $7.99.,2024-07-21 22:00:02\n",
      "\n",
      "84,1,user,Can I buy a fruit basket?,2024-07-22 09:00:00\n",
      "\n",
      "84,2,bot,Yes, we offer pre-made and custom fruit baskets.,2024-07-22 09:00:02\n",
      "\n",
      "85,1,user,Do you have a section for organic produce?,2024-07-22 10:00:00\n",
      "\n",
      "85,2,bot,Yes, we have a dedicated section for organic produce.,2024-07-22 10:00:02\n",
      "\n",
      "86,1,user,Can I get a refund for an item bought on sale?,2024-07-22 11:00:00\n",
      "\n",
      "86,2,bot,Yes, you can get a refund for sale items with a receipt.,2024-07-22 11:00:02\n",
      "\n",
      "87,1,user,What is the price of lemons?,2024-07-22 12:00:00\n",
      "\n",
      "87,2,bot,Lemons are priced at $0.75 each.,2024-07-22 12:00:02\n",
      "\n",
      "88,1,user,Do you have a section for baby products?,2024-07-22 13:00:00\n",
      "\n",
      "88,2,bot,Yes, we have a wide range of baby products available.,2024-07-22 13:00:02\n",
      "\n",
      "89,1,user,Can I get a custom gift basket made?,2024-07-22 14:00:00\n",
      "\n",
      "89,2,bot,Yes, we can create custom gift baskets based on your preferences.,2024-07-22 14:00:02\n",
      "\n",
      "90,1,user,What is the price of organic honey?,2024-07-22 15:00:00\n",
      "\n",
      "90,2,bot,Organic honey is priced at $7.99 per jar.,2024-07-22 15:00:02\n",
      "\n",
      "91,1,user,Do you sell greeting cards?,2024-07-22 16:00:00\n",
      "\n",
      "91,2,bot,Yes, we have a selection of greeting cards for various occasions.,2024-07-22 16:00:02\n",
      "\n",
      "92,1,user,Can I get a discount for being a frequent shopper?,2024-07-22 17:00:00\n",
      "\n",
      "92,2,bot,Yes, our loyalty program offers discounts for frequent shoppers.,2024-07-22 17:00:02\n",
      "\n",
      "93,1,user,Do you offer flu shots?,2024-07-22 18:00:00\n",
      "\n",
      "93,2,bot,Yes, our pharmacy offers flu shots. No appointment is necessary.,2024-07-22 18:00:02\n",
      "\n",
      "94,1,user,What is the price of organic chicken breasts?,2024-07-22 19:00:00\n",
      "\n",
      "94,2,bot,Organic chicken breasts are priced at $6.99 per pound.,2024-07-22 19:00:02\n",
      "\n",
      "95,1,user,Do you have a section for eco-friendly products?,2024-07-22 20:00:00\n",
      "\n",
      "95,2,bot,Yes, we have a dedicated section for eco-friendly products.,2024-07-22 20:00:02\n",
      "\n",
      "96,1,user,Can I buy a gift card online?,2024-07-22 21:00:00\n",
      "\n",
      "96,2,bot,Yes, you can purchase gift cards online.,2024-07-22 21:00:02\n",
      "\n",
      "97,1,user,What is the price of gluten-free bread?,2024-07-22 22:00:00\n",
      "\n",
      "97,2,bot,Gluten-free bread is priced at $4.50 per loaf.,2024-07-22 22:00:02\n",
      "\n",
      "98,1,user,Do you sell frozen yogurt?,2024-07-23 09:00:00\n",
      "\n",
      "98,2,bot,Yes, we have a variety of frozen yogurt flavors available.,2024-07-23 09:00:02\n",
      "\n",
      "99,1,user,Can I get assistance with a heavy item?,2024-07-23 10:00:00\n",
      "\n",
      "99,2,bot,Yes, our staff can assist you with heavy items. Just ask for help at the customer service desk.,2024-07-23 10:00:02\n",
      "\n",
      "100,1,user,What is the price of your deli cheese?,2024-07-23 11:00:00\n",
      "\n",
      "100,2,bot,Prices for deli cheese range from $5.99 to $14.99 per pound depending on the type.,2024-07-23 11:00:02\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Team 1:\n",
      "\n",
      "Date | name | task name | update\n",
      "\n",
      "1-08-2024 Mohamed Faraazman Bin Farooq Chatbot E-D Lstm modified code- \n",
      "\n",
      "\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "from tensorflow.keras.models import Model\n",
      "\n",
      "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
      "\n",
      "from tensorflow.keras.preprocessing.text import Tokenizer\n",
      "\n",
      "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
      "\n",
      "from tensorflow.keras.utils import to_categorical\n",
      "\n",
      "\n",
      "\n",
      "# Load dataset\n",
      "\n",
      "df = pd.read_csv('your_dataset.csv')\n",
      "\n",
      "\n",
      "\n",
      "# Preprocess dataset\n",
      "\n",
      "conversations = []\n",
      "\n",
      "for i in range(0, len(df), 2):\n",
      "\n",
      "    conversations.append([df.iloc[i]['message'], df.iloc[i+1]['message']])\n",
      "\n",
      "\n",
      "\n",
      "input_texts = [conv[0] for conv in conversations]\n",
      "\n",
      "target_texts = ['\\t' + conv[1] + '\\n' for conv in conversations]\n",
      "\n",
      "\n",
      "\n",
      "# Tokenize the input and target texts\n",
      "\n",
      "tokenizer_input = Tokenizer()\n",
      "\n",
      "tokenizer_target = Tokenizer()\n",
      "\n",
      "tokenizer_input.fit_on_texts(input_texts)\n",
      "\n",
      "tokenizer_target.fit_on_texts(target_texts)\n",
      "\n",
      "\n",
      "\n",
      "input_sequences = tokenizer_input.texts_to_sequences(input_texts)\n",
      "\n",
      "target_sequences = tokenizer_target.texts_to_sequences(target_texts)\n",
      "\n",
      "\n",
      "\n",
      "max_encoder_seq_length = max([len(seq) for seq in input_sequences])\n",
      "\n",
      "max_decoder_seq_length = max([len(seq) for seq in target_sequences])\n",
      "\n",
      "\n",
      "\n",
      "input_data = pad_sequences(input_sequences, maxlen=max_encoder_seq_length, padding='post')\n",
      "\n",
      "target_data = pad_sequences(target_sequences, maxlen=max_decoder_seq_length, padding='post')\n",
      "\n",
      "\n",
      "\n",
      "num_encoder_tokens = len(tokenizer_input.word_index) + 1\n",
      "\n",
      "num_decoder_tokens = len(tokenizer_target.word_index) + 1\n",
      "\n",
      "\n",
      "\n",
      "encoder_input_data = to_categorical(input_data, num_encoder_tokens)\n",
      "\n",
      "decoder_input_data = to_categorical(target_data, num_decoder_tokens)\n",
      "\n",
      "\n",
      "\n",
      "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens))\n",
      "\n",
      "for i, seq in enumerate(target_sequences):\n",
      "\n",
      "    for t, token in enumerate(seq):\n",
      "\n",
      "        if t > 0:\n",
      "\n",
      "            decoder_target_data[i, t - 1, token] = 1.0\n",
      "\n",
      "\n",
      "\n",
      "# the model architecture\n",
      "\n",
      "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
      "\n",
      "encoder = LSTM(256, return_state=True)\n",
      "\n",
      "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
      "\n",
      "encoder_states = [state_h, state_c]\n",
      "\n",
      "\n",
      "\n",
      "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
      "\n",
      "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
      "\n",
      "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
      "\n",
      "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
      "\n",
      "decoder_outputs = decoder_dense(decoder_outputs)\n",
      "\n",
      "\n",
      "\n",
      "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
      "\n",
      "\n",
      "\n",
      "# Compile and train the model\n",
      "\n",
      "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
      "\n",
      "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
      "\n",
      "          batch_size=64, epochs=100, validation_split=0.2)\n",
      "\n",
      "\n",
      "\n",
      "# Defines the inference models\n",
      "\n",
      "encoder_model = Model(encoder_inputs, encoder_states)\n",
      "\n",
      "\n",
      "\n",
      "decoder_state_input_h = Input(shape=(256,))\n",
      "\n",
      "decoder_state_input_c = Input(shape=(256,))\n",
      "\n",
      "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
      "\n",
      "\n",
      "\n",
      "decoder_outputs, state_h, state_c = decoder_lstm(\n",
      "\n",
      "    decoder_inputs, initial_state=decoder_states_inputs)\n",
      "\n",
      "decoder_states = [state_h, state_c]\n",
      "\n",
      "decoder_outputs = decoder_dense(decoder_outputs)\n",
      "\n",
      "decoder_model = Model(\n",
      "\n",
      "    [decoder_inputs] + decoder_states_inputs,\n",
      "\n",
      "    [decoder_outputs] + decoder_states)\n",
      "\n",
      "\n",
      "\n",
      "def decode_sequence(input_seq):\n",
      "\n",
      "    states_value = encoder_model.predict(input_seq)\n",
      "\n",
      "\n",
      "\n",
      "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
      "\n",
      "    target_seq[0, 0, tokenizer_target.word_index['\\t']] = 1.0\n",
      "\n",
      "\n",
      "\n",
      "    stop_condition = False\n",
      "\n",
      "    decoded_sentence = ''\n",
      "\n",
      "    while not stop_condition:\n",
      "\n",
      "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
      "\n",
      "\n",
      "\n",
      "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
      "\n",
      "        sampled_char = tokenizer_target.index_word[sampled_token_index]\n",
      "\n",
      "        decoded_sentence += sampled_char\n",
      "\n",
      "\n",
      "\n",
      "        if sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length:\n",
      "\n",
      "            stop_condition = True\n",
      "\n",
      "\n",
      "\n",
      "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
      "\n",
      "        target_seq[0, 0, sampled_token_index] = 1.0\n",
      "\n",
      "\n",
      "\n",
      "        states_value = [h, c]\n",
      "\n",
      "\n",
      "\n",
      "    return decoded_sentence\n",
      "\n",
      "\n",
      "\n",
      "# Testing the model\n",
      "\n",
      "for seq_index in range(10):\n",
      "\n",
      "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
      "\n",
      "    decoded_sentence = decode_sequence(input_seq)\n",
      "\n",
      "    print('Input:', input_texts[seq_index])\n",
      "\n",
      "    print('Decoded:', decoded_sentence)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Team 2:\n",
      "\n",
      "Date | name | task name | update' metadata={'source': './docs/project-chatbot.docx'}\n",
      "page_content='Mohamed Faraazman bin Farooq S  \n",
      "As a fourth -year student majoring in Artificial Intelligence and Data Science at BSA Crescent \n",
      "University, Chennai, with a CGPA of 9.1/10, I have cultivated a profound interest in the \n",
      "interdisciplinary applications of AI and data science. This interest has been the driving force \n",
      "behind my academic pursuits, hands -on projects, and research endeavors, all of which have \n",
      "prepared me for the next pivotal step in my career: an internship at KAUST's Visiting Student \n",
      "Research Program.  \n",
      "From an early age, I have been fascinated by the mysteries of the universe and the potential of \n",
      "technology to unravel them. This fascination led me to pursue a B.Tech in Artificial Intelligence \n",
      "and Data Science, where I have honed my skills in Python (Scik it-learn, Pandas, PyTorch, \n",
      "TensorFlow), Java, C, R, HTML, CSS, MS Office, Power BI, Tableau, SQL, and Android App \n",
      "Development. My coursework has provided a solid foundation in Data Structures and \n",
      "Algorithms, Computer Networks, Operating Systems, DBMS, and Cloud Computing, equipping \n",
      "me with the technical expertise required to tackle complex problems.  \n",
      "One of my most significant academic projects involved developing a framework for personalized \n",
      "medicine using DNA sequencing analysis in Python. This project not only sharpened my \n",
      "programming skills but also deepened my understanding of how data science can  be leveraged to \n",
      "make impactful contributions to the field of healthcare. The experience of working on this \n",
      "project reinforced my belief in the transformative power of data -driven solutions and motivated \n",
      "me to explore further applications of AI and data sc ience in other domains, particularly in \n",
      "biological research.  \n",
      "During my studies, I have also gained practical experience through internships and projects. My \n",
      "proficiency with tools such as Github, Linux, Windows, VS Code, Android Studio, and Jupyter \n",
      "Notebook has enabled me to collaborate effectively in team settings and contribute to diverse \n",
      "projects. These experiences have taught me the importance of teamwork, problem -solving, and \n",
      "communication skills, which are crucial for success in any professional setting.  \n",
      "I am particularly interested in projects involving networks or graphs, especially biological ones. \n",
      "Challenges such as comparing multiple networks, embedding networks into hyperbolic space, \n",
      "integrating networks into machine learning frameworks, generative m odeling of networks, \n",
      "analyzing partially overlapping networks, search and propagation in multi -layer networks, and \n",
      "aligning different real protein interaction networks are areas where I am eager to contribute. \n",
      "These complex problems align perfectly with my  skills and interests, and I am excited about the \n",
      "opportunity to work on them in a collaborative research environment.  \n",
      "KAUST's Visiting Student Research Program represents an extraordinary opportunity for me to \n",
      "apply my skills in a research environment that aligns perfectly with my interests. The chance to \n",
      "work on cutting -edge projects related to network analysis, data sci ence, and AI -driven insights is \n",
      "immensely exciting. I am particularly eager to contribute to the development of innovative tools \n",
      "and methods that can enhance our understanding of complex biological systems.  ' metadata={'source': './docs/SOP.pdf', 'page': 0}\n",
      "page_content='Furthermore, I am enthusiastic about the prospect of learning from and collaborating with the \n",
      "distinguished researchers and scientists at KAUST. Their expertise and guidance will be \n",
      "invaluable in helping me refine my technical skills, expand my knowledge, and develop a more \n",
      "nuanced understanding of the intersection between AI and biological research.  \n",
      "In conclusion, my academic background, technical skills, and passion for data science make me a \n",
      "strong candidate for KAUST's Visiting Student Research Program. I am confident that this \n",
      "internship will be a transformative experience, enabling me to contribu te meaningfully to the \n",
      "field of network analysis and furthering my goal of becoming a leading researcher in the \n",
      "application of AI and data science to solve some of the most challenging questions in biology.  \n",
      "Thank you for considering my application.  \n",
      "Sincerely,  \n",
      "Mohamed Faraazman bin Farooq S  \n",
      "md.faraazman@gmail.com  \n",
      " ' metadata={'source': './docs/SOP.pdf', 'page': 1}\n",
      "page_content='Mohamed Faraazman bin Farooq S  \n",
      "As a fourth -year student majoring in Artificial Intelligence and Data Science at BSA Crescent \n",
      "University, Chennai, with a CGPA of 9.1/10, I have cultivated a profound interest in the \n",
      "interdisciplinary applications of AI and data science. This interest has been the driving force \n",
      "behind my academic pursuits, hands -on projects, and research endeavors, all of which have \n",
      "prepared me for the next pivotal step in my career: an internship at KAUST's Visiting Student \n",
      "Research Program.  \n",
      "From an early age, I have been fascinated by the mysteries of the universe and the potential of \n",
      "technology to unravel them. This fascination led me to pursue a B.Tech in Artificial Intelligence \n",
      "and Data Science, where I have honed my skills in Python (Scik it-learn, Pandas, PyTorch, \n",
      "TensorFlow), Java, C, R, HTML, CSS, MS Office, Power BI, Tableau, SQL, and Android App \n",
      "Development. My coursework has provided a solid foundation in Data Structures and \n",
      "Algorithms, Computer Networks, Operating Systems, DBMS, and Cloud Computing, equipping \n",
      "me with the technical expertise required to tackle complex problems.  \n",
      "One of my most significant academic projects involved developing a framework for personalized \n",
      "medicine using DNA sequencing analysis in Python. This project not only sharpened my \n",
      "programming skills but also deepened my understanding of how data science can  be leveraged to \n",
      "make impactful contributions to the field of healthcare. The experience of working on this \n",
      "project reinforced my belief in the transformative power of data -driven solutions and motivated \n",
      "me to explore further applications of AI and data sc ience in other domains, particularly in \n",
      "biological research.  \n",
      "During my studies, I have also gained practical experience through internships and projects. My \n",
      "proficiency with tools such as Github, Linux, Windows, VS Code, Android Studio, and Jupyter \n",
      "Notebook has enabled me to collaborate effectively in team settings and contribute to diverse \n",
      "projects. These experiences have taught me the importance of teamwork, problem -solving, and \n",
      "communication skills, which are crucial for success in any professional setting.  \n",
      "I am particularly interested in projects involving networks or graphs, especially biological ones. \n",
      "Challenges such as comparing multiple networks, embedding networks into hyperbolic space, \n",
      "integrating networks into machine learning frameworks, generative m odeling of networks, \n",
      "analyzing partially overlapping networks, search and propagation in multi -layer networks, and \n",
      "aligning different real protein interaction networks are areas where I am eager to contribute. \n",
      "These complex problems align perfectly with my  skills and interests, and I am excited about the \n",
      "opportunity to work on them in a collaborative research environment.  \n",
      "KAUST's Visiting Student Research Program represents an extraordinary opportunity for me to \n",
      "apply my skills in a research environment that aligns perfectly with my interests. The chance to \n",
      "work on cutting -edge projects related to network analysis, data sci ence, and AI -driven insights is \n",
      "immensely exciting. I am particularly eager to contribute to the development of innovative tools \n",
      "and methods that can enhance our understanding of complex biological systems.  ' metadata={'source': './docs/SOP.pdf', 'page': 0}\n",
      "page_content='Furthermore, I am enthusiastic about the prospect of learning from and collaborating with the \n",
      "distinguished researchers and scientists at KAUST. Their expertise and guidance will be \n",
      "invaluable in helping me refine my technical skills, expand my knowledge, and develop a more \n",
      "nuanced understanding of the intersection between AI and biological research.  \n",
      "In conclusion, my academic background, technical skills, and passion for data science make me a \n",
      "strong candidate for KAUST's Visiting Student Research Program. I am confident that this \n",
      "internship will be a transformative experience, enabling me to contribu te meaningfully to the \n",
      "field of network analysis and furthering my goal of becoming a leading researcher in the \n",
      "application of AI and data science to solve some of the most challenging questions in biology.  \n",
      "Thank you for considering my application.  \n",
      "Sincerely,  \n",
      "Mohamed Faraazman bin Farooq S  \n",
      "md.faraazman@gmail.com  \n",
      " ' metadata={'source': './docs/SOP.pdf', 'page': 1}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "\n",
    "documents = []\n",
    "for file in os.listdir('docs'):\n",
    "    if file.endswith('.pdf'):\n",
    "        loader = PyPDFLoader('./docs/' + file)\n",
    "    elif file.endswith('.docx'):\n",
    "        loader = Docx2txtLoader('./docs/' + file)\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "# Print the loaded documents for verification\n",
    "for doc in documents:\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ytn5zcCsH6_9",
    "outputId": "55eaf2e9-dc7f-417f-ab09-16bcdadec1c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.12)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.27 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.28)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.98)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.3.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (4.12.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.27->langchain) (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VuEP8WmVImUN",
    "outputId": "e2c00047-8596-4e04-f9b7-aa8548ea9f67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Team 1 \n",
      "\n",
      "Mohamed Faraazman Bin Farooq S | Linga vara Prasad\n",
      "\n",
      "Task name LSTM\n",
      "\n",
      "\n",
      "\n",
      "# Preprocess Your Data\n",
      "\n",
      "from tensorflow.keras.preprocessing.text import Tokenizer\n",
      "\n",
      "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
      "\n",
      "from tensorflow.keras.utils import to_categorical\n",
      "\n",
      "\n",
      "\n",
      "# Sample data\n",
      "\n",
      "input_texts = [\"How much is this product?\", \"Where is the dairy section?\"]\n",
      "\n",
      "target_texts = [\"This product costs $5.\", \"The dairy section is in aisle 3.\"]\n",
      "\n",
      "\n",
      "\n",
      "# Tokenize\n",
      "\n",
      "tokenizer_input = Tokenizer()\n",
      "\n",
      "tokenizer_input.fit_on_texts(input_texts)\n",
      "\n",
      "input_sequences = tokenizer_input.texts_to_sequences(input_texts)\n",
      "\n",
      "\n",
      "\n",
      "tokenizer_target = Tokenizer()\n",
      "\n",
      "tokenizer_target.fit_on_texts(target_texts)\n",
      "\n",
      "target_sequences = tokenizer_target.texts_to_sequences(target_texts)\n",
      "\n",
      "\n",
      "\n",
      "# Convert to one-hot encoded vectors\n",
      "\n",
      "max_encoder_seq_length = max(len(seq) for seq in input_sequences)\n",
      "\n",
      "max_decoder_seq_length = max(len(seq) for seq in target_sequences)\n",
      "\n",
      "num_encoder_tokens = len(tokenizer_input.word_index) + 1\n",
      "\n",
      "num_decoder_tokens = len(tokenizer_target.word_index) + 1\n",
      "\n",
      "\n",
      "\n",
      "encoder_input_data = pad_sequences(input_sequences, maxlen=max_encoder_seq_length)\n",
      "\n",
      "decoder_input_data = pad_sequences(target_sequences, maxlen=max_decoder_seq_length)\n",
      "\n",
      "decoder_target_data = np.zeros((len(target_sequences), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
      "\n",
      "\n",
      "\n",
      "for i, seq in enumerate(target_sequences):\n",
      "\n",
      "    for t, token in enumerate(seq):\n",
      "\n",
      "        if t > 0:\n",
      "\n",
      "            decoder_target_data[i, t - 1, token] = 1.0\n",
      "\n",
      "# Build the Seq2Seq Model   \n",
      "\n",
      "from tensorflow.keras.models import Model\n",
      "\n",
      "from tensorflow.keras.layers import Input, LSTM, Dense\n",
      "\n",
      "\n",
      "\n",
      "# Encoder\n",
      "\n",
      "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
      "\n",
      "encoder = LSTM(256, return_state=True)\n",
      "\n",
      "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
      "\n",
      "encoder_states = [state_h, state_c]\n",
      "\n",
      "\n",
      "\n",
      "# Decoder\n",
      "\n",
      "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
      "\n",
      "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
      "\n",
      "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
      "\n",
      "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
      "\n",
      "decoder_outputs = decoder_dense(decoder_outputs)\n",
      "\n",
      "\n",
      "\n",
      "# Model\n",
      "\n",
      "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
      "\n",
      "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
      "\n",
      "\n",
      "\n",
      "# Train the model\n",
      "\n",
      "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=64, epochs=100, validation_split=0.2)\n",
      "\n",
      "# Define Inference Models\n",
      "\n",
      "# Encoder model for inference\n",
      "\n",
      "encoder_model = Model(encoder_inputs, encoder_states)\n",
      "\n",
      "\n",
      "\n",
      "# Decoder model for inference\n",
      "\n",
      "decoder_state_input_h = Input(shape=(256,))\n",
      "\n",
      "decoder_state_input_c = Input(shape=(256,))\n",
      "\n",
      "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
      "\n",
      "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
      "\n",
      "decoder_states = [state_h, state_c]\n",
      "\n",
      "decoder_outputs = decoder_dense(decoder_outputs)\n",
      "\n",
      "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
      "\n",
      "# Create a Response Function\n",
      "\n",
      "def decode_sequence(input_seq):\n",
      "\n",
      "    states_value = encoder_model.predict(input_seq)\n",
      "\n",
      "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
      "\n",
      "    target_seq[0, 0, tokenizer_target.word_index['\\t']] = 1.0  # Start token\n",
      "\n",
      "\n",
      "\n",
      "    stop_condition = False\n",
      "\n",
      "    decoded_sentence = ''\n",
      "\n",
      "    while not stop_condition:\n",
      "\n",
      "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
      "\n",
      "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
      "\n",
      "        sampled_char = tokenizer_target.index_word[sampled_token_index]\n",
      "\n",
      "\n",
      "\n",
      "        if sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length:\n",
      "\n",
      "            stop_condition = True\n",
      "\n",
      "\n",
      "\n",
      "        decoded_sentence += ' ' + sampled_char\n",
      "\n",
      "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
      "\n",
      "        target_seq[0, 0, sampled_token_index] = 1.0\n",
      "\n",
      "        states_value = [h, c]\n",
      "\n",
      "\n",
      "\n",
      "    return decoded_sentence\n",
      "\n",
      "# Test the Model\n",
      "\n",
      "for seq_index in range(len(input_texts)):\n",
      "\n",
      "    input_seq = pad_sequences([input_sequences[seq_index]], maxlen=max_encoder_seq_length)\n",
      "\n",
      "    decoded_sentence = decode_sequence(input_seq)\n",
      "\n",
      "    print(f'Input: {input_texts[seq_index]}')\n",
      "\n",
      "    print(f'Decoded: {decoded_sentence}')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Team 2\n",
      "\n",
      "Ankit kumar | \n",
      "\n",
      "Task name transformer\n",
      "\n",
      "\n",
      "\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.keras.layers import Input, Dense, Embedding, Dropout, LayerNormalization, MultiHeadAttention\n",
      "\n",
      "from tensorflow.keras.models import Model\n",
      "\n",
      "\n",
      "\n",
      "# Function to define a Transformer Encoder layer\n",
      "\n",
      "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    Applies the Transformer Encoder layer which includes multi-head self-attention\n",
      "\n",
      "    and feed-forward neural network.\n",
      "\n",
      "\n",
      "\n",
      "    Args:\n",
      "\n",
      "    - inputs: Input tensor to the encoder layer.\n",
      "\n",
      "    - head_size: Dimensionality of the attention heads.\n",
      "\n",
      "    - num_heads: Number of attention heads.\n",
      "\n",
      "    - ff_dim: Dimensionality of the feed-forward layer.\n",
      "\n",
      "    - dropout: Dropout rate for regularization.\n",
      "\n",
      "\n",
      "\n",
      "    Returns:\n",
      "\n",
      "    - Output tensor after applying the encoder layer.\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    # Layer Normalization and Self-Attention\n",
      "\n",
      "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
      "\n",
      "    x = MultiHeadAttention(\n",
      "\n",
      "        key_dim=head_size, \n",
      "\n",
      "        num_heads=num_heads, \n",
      "\n",
      "        dropout=dropout\n",
      "\n",
      "    )(x, x)\n",
      "\n",
      "    x = Dropout(dropout)(x)\n",
      "\n",
      "    # Residual Connection\n",
      "\n",
      "    res = x + inputs\n",
      "\n",
      "\n",
      "\n",
      "    # Feed Forward Network\n",
      "\n",
      "    x = LayerNormalization(epsilon=1e-6)(res)\n",
      "\n",
      "    x = Dense(ff_dim, activation=\"relu\")(x)\n",
      "\n",
      "    x = Dropout(dropout)(x)\n",
      "\n",
      "    x = Dense(inputs.shape[-1])(x)\n",
      "\n",
      "    return x + res\n",
      "\n",
      "\n",
      "\n",
      "# Function to define a Transformer Decoder layer\n",
      "\n",
      "def transformer_decoder(target, context, head_size, num_heads, ff_dim, dropout=0):\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    Applies the Transformer Decoder layer which includes multi-head self-attention,\n",
      "\n",
      "    cross-attention with encoder outputs, and feed-forward neural network.\n",
      "\n",
      "\n",
      "\n",
      "    Args:\n",
      "\n",
      "    - target: Input tensor to the decoder layer (usually the target sequence).\n",
      "\n",
      "    - context: Encoder output that provides context for decoding.\n",
      "\n",
      "    - head_size: Dimensionality of the attention heads.\n",
      "\n",
      "    - num_heads: Number of attention heads.\n",
      "\n",
      "    - ff_dim: Dimensionality of the feed-forward layer.\n",
      "\n",
      "    - dropout: Dropout rate for regularization.\n",
      "\n",
      "\n",
      "\n",
      "    Returns:\n",
      "\n",
      "    - Output tensor after applying the decoder layer.\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    # Layer Normalization and Self-Attention\n",
      "\n",
      "    x = LayerNormalization(epsilon=1e-6)(target)\n",
      "\n",
      "    x = MultiHeadAttention(\n",
      "\n",
      "        key_dim=head_size, \n",
      "\n",
      "        num_heads=num_heads, \n",
      "\n",
      "        dropout=dropout\n",
      "\n",
      "    )(x, x)\n",
      "\n",
      "    x = Dropout(dropout)(x)\n",
      "\n",
      "    x = x + target\n",
      "\n",
      "\n",
      "\n",
      "    # Layer Normalization and Cross-Attention with Encoder Output\n",
      "\n",
      "    x = LayerNormalization(epsilon=1e-6)(x)\n",
      "\n",
      "    x = MultiHeadAttention(\n",
      "\n",
      "        key_dim=head_size, \n",
      "\n",
      "        num_heads=num_heads, \n",
      "\n",
      "        dropout=dropout\n",
      "\n",
      "    )(x, context)\n",
      "\n",
      "    x = Dropout(dropout)(x)\n",
      "\n",
      "    x = x + target\n",
      "\n",
      "\n",
      "\n",
      "    # Feed Forward Network\n",
      "\n",
      "    x = LayerNormalization(epsilon=1e-6)(x)\n",
      "\n",
      "    x = Dense(ff_dim, activation=\"relu\")(x)\n",
      "\n",
      "    x = Dropout(dropout)(x)\n",
      "\n",
      "    x = Dense(target.shape[-1])(x)\n",
      "\n",
      "    return x + target\n",
      "\n",
      "\n",
      "\n",
      "# Function to build the full Transformer model\n",
      "\n",
      "def build_transformer(vocab_size, max_len, num_layers, head_size, num_heads, ff_dim, dropout=0):\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    Builds the complete Transformer model with both encoder and decoder.\n",
      "\n",
      "\n",
      "\n",
      "    Args:\n",
      "\n",
      "    - vocab_size: Size of the vocabulary (number of unique tokens).\n",
      "\n",
      "    - max_len: Maximum length of input and output sequences.\n",
      "\n",
      "    - num_layers: Number of Transformer layers in both encoder and decoder.\n",
      "\n",
      "    - head_size: Dimensionality of the attention heads.\n",
      "\n",
      "    - num_heads: Number of attention heads.\n",
      "\n",
      "    - ff_dim: Dimensionality of the feed-forward layers.\n",
      "\n",
      "    - dropout: Dropout rate for regularization.\n",
      "\n",
      "\n",
      "\n",
      "    Returns:\n",
      "\n",
      "    - The Transformer model instance.\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    # Input for encoder\n",
      "\n",
      "    inputs = Input(shape=(max_len,))\n",
      "\n",
      "    x = Embedding(vocab_size, head_size)(inputs)\n",
      "\n",
      "    for _ in range(num_layers):\n",
      "\n",
      "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
      "\n",
      "    \n",
      "\n",
      "    # Input for decoder\n",
      "\n",
      "    targets = Input(shape=(max_len,))\n",
      "\n",
      "    y = Embedding(vocab_size, head_size)(targets)\n",
      "\n",
      "    for _ in range(num_layers):\n",
      "\n",
      "        y = transformer_decoder(y, x, head_size, num_heads, ff_dim, dropout)\n",
      "\n",
      "    \n",
      "\n",
      "    # Output layer\n",
      "\n",
      "    outputs = Dense(vocab_size, activation='softmax')(y)\n",
      "\n",
      "\n",
      "\n",
      "    # Create the model\n",
      "\n",
      "    model = Model(inputs=[inputs, targets], outputs=outputs)\n",
      "\n",
      "    return model\n",
      "\n",
      "\n",
      "\n",
      "# Define model parameters\n",
      "\n",
      "vocab_size = 10000  # Size of vocabulary\n",
      "\n",
      "max_len = 50        # Maximum length of input and output sequences\n",
      "\n",
      "num_layers = 4      # Number of Transformer layers\n",
      "\n",
      "head_size = 64      # Dimensionality of the attention heads\n",
      "\n",
      "num_heads = 4       # Number of attention heads\n",
      "\n",
      "ff_dim = 128        # Dimensionality of the feed-forward layers\n",
      "\n",
      "dropout = 0.1       # Dropout rate\n",
      "\n",
      "\n",
      "\n",
      "# Build the model\n",
      "\n",
      "model = build_transformer(vocab_size, max_len, num_layers, head_size, num_heads, ff_dim, dropout)\n",
      "\n",
      "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
      "\n",
      "\n",
      "\n",
      "# Print model summary\n",
      "\n",
      "model.summary()\n",
      "\n",
      "\n",
      "\n",
      "# Example dummy data for training\n",
      "\n",
      "X_train = [np.random.randint(0, vocab_size, (1000, max_len)), np.random.randint(0, vocab_size, (1000, max_len))]\n",
      "\n",
      "y_train = np.random.randint(0, vocab_size, (1000, max_len))\n",
      "\n",
      "\n",
      "\n",
      "# Train the model\n",
      "\n",
      "model.fit(X_train, y_train, epochs=1, batch_size=32)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset\n",
      "\n",
      "conversation_id,turn_id,sender,message,timestamp\n",
      "\n",
      "1,1,user,Hello,2024-07-16 10:00:00\n",
      "\n",
      "1,2,bot,Hi! How can I help you today?,2024-07-16 10:00:02\n",
      "\n",
      "1,3,user,Can you tell me if you have organic apples in stock?,2024-07-16 10:00:05\n",
      "\n",
      "1,4,bot,Yes, we have organic apples in stock. Anything else I can help you with?,2024-07-16 10:00:07\n",
      "\n",
      "2,1,user,What are your store hours?,2024-07-16 11:00:00\n",
      "\n",
      "2,2,bot,We are open from 8 AM to 10 PM every day.,2024-07-16 11:00:02\n",
      "\n",
      "3,1,user,Do you have any discounts on dairy products?,2024-07-16 12:00:00\n",
      "\n",
      "3,2,bot,Yes, we currently have a 10% discount on all dairy products.,2024-07-16 12:00:02\n",
      "\n",
      "4,1,user,Can you help me find gluten-free bread?,2024-07-16 13:00:00\n",
      "\n",
      "4,2,bot,Sure, we have gluten-free bread in aisle 5.,2024-07-16 13:00:02\n",
      "\n",
      "5,1,user,What is the price of your organic milk?,2024-07-16 14:00:00\n",
      "\n",
      "5,2,bot,Organic milk is priced at $3.99 per liter.,2024-07-16 14:00:02\n",
      "\n",
      "6,1,user,Do you offer home delivery?,2024-07-16 15:00:00\n",
      "\n",
      "6,2,bot,Yes, we offer home delivery for orders over $50.,2024-07-16 15:00:02\n",
      "\n",
      "7,1,user,How can I return a product?,2024-07-16 16:00:00\n",
      "\n",
      "7,2,bot,You can return a product within 30 days of purchase with a receipt.,2024-07-16 16:00:02\n",
      "\n",
      "8,1,user,Are there any vegan options available?,2024-07-16 17:00:00\n",
      "\n",
      "8,2,bot,Yes, we have a variety of vegan options available in the store.,2024-07-16 17:00:02\n",
      "\n",
      "9,1,user,Can I pay with digital wallets?,2024-07-16 18:00:00\n",
      "\n",
      "9,2,bot,Yes, we accept payments via digital wallets such as Apple Pay and Google Pay.,2024-07-16 18:00:02\n",
      "\n",
      "10,1,user,Where can I find the bakery section?,2024-07-16 19:00:00\n",
      "\n",
      "10,2,bot,The bakery section is located near the entrance of the store.,2024-07-16 19:00:02\n",
      "\n",
      "11,1,user,Do you have fresh fish available?,2024-07-16 20:00:00\n",
      "\n",
      "11,2,bot,Yes, we have a variety of fresh fish available daily.,2024-07-16 20:00:02\n",
      "\n",
      "12,1,user,What is your return policy on perishables?,2024-07-16 21:00:00\n",
      "\n",
      "12,2,bot,Perishables can be returned within 7 days of purchase with a receipt.,2024-07-16 21:00:02\n",
      "\n",
      "13,1,user,Can I order groceries online?,2024-07-16 22:00:00\n",
      "\n",
      "13,2,bot,Yes, you can place orders on our website and choose home delivery or store pickup.,2024-07-16 22:00:02\n",
      "\n",
      "14,1,user,Do you have a loyalty program?,2024-07-17 09:00:00\n",
      "\n",
      "14,2,bot,Yes, we have a loyalty program where you can earn points for every purchase.,2024-07-17 09:00:02\n",
      "\n",
      "15,1,user,Can I get assistance with carrying my groceries to my car?,2024-07-17 10:00:00\n",
      "\n",
      "15,2,bot,Yes, our staff can assist you with carrying groceries to your car.,2024-07-17 10:00:02\n",
      "\n",
      "16,1,user,What are the benefits of your loyalty program?,2024-07-17 11:00:00\n",
      "\n",
      "16,2,bot,You can earn points on purchases, get exclusive discounts, and receive special offers.,2024-07-17 11:00:02\n",
      "\n",
      "17,1,user,Do you sell gift cards?,2024-07-17 12:00:00\n",
      "\n",
      "17,2,bot,Yes, we sell gift cards in various denominations.,2024-07-17 12:00:02\n",
      "\n",
      "18,1,user,Is there a pharmacy in your store?,2024-07-17 13:00:00\n",
      "\n",
      "18,2,bot,Yes, we have a pharmacy in our store.,2024-07-17 13:00:02\n",
      "\n",
      "19,1,user,Do you offer any cooking classes?,2024-07-17 14:00:00\n",
      "\n",
      "19,2,bot,Yes, we offer cooking classes. You can check our schedule on the website.,2024-07-17 14:00:02\n",
      "\n",
      "20,1,user,Can I place a bulk order for a party?,2024-07-17 15:00:00\n",
      "\n",
      "20,2,bot,Yes, you can place bulk orders by contacting our customer service.,2024-07-17 15:00:02\n",
      "\n",
      "21,1,user,Do you have a section for international foods?,2024-07-17 16:00:00\n",
      "\n",
      "21,2,bot,Yes, we have an aisle dedicated to international foods.,2024-07-17 16:00:02\n",
      "\n",
      "22,1,user,Can I get a refund if the product is expired?,2024-07-17 17:00:00\n",
      "\n",
      "22,2,bot,Yes, you can get a refund or exchange if the product is expired.,2024-07-17 17:00:02\n",
      "\n",
      "23,1,user,Do you have organic vegetables?,2024-07-17 18:00:00\n",
      "\n",
      "23,2,bot,Yes, we have a wide range of organic vegetables available.,2024-07-17 18:00:02\n",
      "\n",
      "24,1,user,Can I schedule a delivery for a specific time?,2024-07-17 19:00:00\n",
      "\n",
      "24,2,bot,Yes, you can choose a delivery slot that fits your schedule.,2024-07-17 19:00:02\n",
      "\n",
      "25,1,user,What forms of payment do you accept?,2024-07-17 20:00:00\n",
      "\n",
      "25,2,bot,We accept cash, credit/debit cards, and digital wallets.,2024-07-17 20:00:02\n",
      "\n",
      "26,1,user,Do you have a bakery?,2024-07-17 21:00:00\n",
      "\n",
      "26,2,bot,Yes, we have a bakery that offers fresh bread and pastries.,2024-07-17 21:00:02\n",
      "\n",
      "27,1,user,What is the price of avocados?,2024-07-17 22:00:00\n",
      "\n",
      "27,2,bot,Avocados are priced at $1.50 each.,2024-07-17 22:00:02\n",
      "\n",
      "28,1,user,Do you sell non-dairy milk?,2024-07-18 09:00:00\n",
      "\n",
      "28,2,bot,Yes, we have a variety of non-dairy milk options including almond, soy, and oat milk.,2024-07-18 09:00:02\n",
      "\n",
      "29,1,user,Where can I find cleaning supplies?,2024-07-18 10:00:00\n",
      "\n",
      "29,2,bot,Cleaning supplies are located in aisle 7.,2024-07-18 10:00:02\n",
      "\n",
      "30,1,user,Do you have a customer service desk?,2024-07-18 11:00:00\n",
      "\n",
      "30,2,bot,Yes, our customer service desk is located near the store entrance.,2024-07-18 11:00:02\n",
      "\n",
      "31,1,user,Can I buy alcohol in your store?,2024-07-18 12:00:00\n",
      "\n",
      "31,2,bot,Yes, we have a wide selection of alcoholic beverages.,2024-07-18 12:00:02\n",
      "\n",
      "32,1,user,What is your policy on price matching?,2024-07-18 13:00:00\n",
      "\n",
      "32,2,bot,We offer price matching on identical items with proof of a lower price.,2024-07-18 13:00:02\n",
      "\n",
      "33,1,user,Do you sell pet food?,2024-07-18 14:00:00\n",
      "\n",
      "33,2,bot,Yes, we have a variety of pet food and supplies.,2024-07-18 14:00:02\n",
      "\n",
      "34,1,user,Can I get a rain check for a sale item?,2024-07-18 15:00:00\n",
      "\n",
      "34,2,bot,Yes, we offer rain checks for out-of-stock sale items.,2024-07-18 15:00:02\n",
      "\n",
      "35,1,user,Do you offer senior discounts?,2024-07-18 16:00:00\n",
      "\n",
      "35,2,bot,Yes, we offer a 5% discount for seniors on Tuesdays.,2024-07-18 16:00:02\n",
      "\n",
      "36,1,user,Are there any gluten-free pasta options?,2024-07-18 17:00:00\n",
      "\n",
      "36,2,bot,Yes, we have several gluten-free pasta options.,2024-07-18 17:00:02\n",
      "\n",
      "37,1,user,What is the price of bananas?,2024-07-18 18:00:00\n",
      "\n",
      "37,2,bot,Bananas are priced at $0.99 per pound.,2024-07-18 18:00:02\n",
      "\n",
      "38,1,user,Can I buy bulk spices?,2024-07-18 19:00:00\n",
      "\n",
      "38,2,bot,Yes, we offer a variety of spices in bulk.,2024-07-18 19:00:02\n",
      "\n",
      "39,1,user,Do you have a florist section?,2024-07-18 20:00:00\n",
      "\n",
      "39,2,bot,Yes, we have a florist section with fresh flowers and arrangements.,2024-07-18 20:00:02\n",
      "\n",
      "40,1,user,What is the price of a gallon of water?,2024-07-18 21:00:00\n",
      "\n",
      "40,2,bot,A gallon of water is priced at $0.89.,2024-07-18 21:00:02\n",
      "\n",
      "41,1,user,Do you have a butcher?,2024-07-18 22:00:00\n",
      "\n",
      "41,2,bot,Yes, we have a butcher who can assist with custom cuts of meat.,2024-07-18 22:00:02\n",
      "\n",
      "42,1,user,Can I recycle plastic bags in your store?,2024-07-19 09:00:00\n",
      "\n",
      "42,2,bot,Yes, we have a recycling bin for plastic bags near the entrance.,2024-07-19 09:00:02\n",
      "\n",
      "43,1,user,Do you have self-checkout kiosks?,2024-07-19 10:00:00\n",
      "\n",
      "43,2,bot,Yes, we have self-checkout kiosks for your convenience.,2024-07-19 10:00:02\n",
      "\n",
      "44,1,user,Can I get a custom cake made?,2024-07-19 11:00:00\n",
      "\n",
      "44,2,bot,Yes, our bakery can make custom cakes to order.,2024-07-19 11:00:02\n",
      "\n",
      "45,1,user,Do you sell batteries?,2024-07-19 12:00:00\n",
      "\n",
      "45,2,bot,Yes, we sell a variety of batteries.,2024-07-19 12:00:02\n",
      "\n",
      "46,1,user,Can I order a deli platter?,2024-07-19 13:00:00\n",
      "\n",
      "46,2,bot,Yes, you can order a deli platter by contacting our deli department.,2024-07-19 13:00:02\n",
      "\n",
      "47,1,user,What is the price of ground beef?,2024-07-19 14:00:00\n",
      "\n",
      "47,2,bot,Ground beef is priced at $4.99 per pound.,2024-07-19 14:00:02\n",
      "\n",
      "48,1,user,Do you have a section for bulk items?,2024-07-19 15:00:00\n",
      "\n",
      "48,2,bot,Yes, we have a section dedicated to bulk items.,2024-07-19 15:00:02\n",
      "\n",
      "49,1,user,What is the price of eggs?,2024-07-19 16:00:00\n",
      "\n",
      "49,2,bot,A dozen eggs are priced at $2.99.,2024-07-19 16:00:02\n",
      "\n",
      "50,1,user,Do you sell baby products?,2024-07-19 17:00:00\n",
      "\n",
      "50,2,bot,Yes, we have a range of baby products available.,2024-07-19 17:00:02\n",
      "\n",
      "51,1,user,Can I get a refund for a damaged item?,2024-07-19 18:00:00\n",
      "\n",
      "51,2,bot,Yes, you can get a refund or exchange for a damaged item with a receipt.,2024-07-19 18:00:02\n",
      "\n",
      "52,1,user,Do you offer curbside pickup?,2024-07-19 19:00:00\n",
      "\n",
      "52,2,bot,Yes, we offer curbside pickup for online orders.,2024-07-19 19:00:02\n",
      "\n",
      "53,1,user,What is the price of your deli sandwiches?,2024-07-19 20:00:00\n",
      "\n",
      "53,2,bot,Deli sandwiches are priced between $5.99 and $8.99 depending on the type.,2024-07-19 20:00:02\n",
      "\n",
      "54,1,user,Can I buy prepared meals?,2024-07-19 21:00:00\n",
      "\n",
      "54,2,bot,Yes, we have a selection of prepared meals in our deli section.,2024-07-19 21:00:02\n",
      "\n",
      "55,1,user,Do you have a cafe?,2024-07-19 22:00:00\n",
      "\n",
      "55,2,bot,Yes, we have a cafe that serves coffee, tea, and light snacks.,2024-07-19 22:00:02\n",
      "\n",
      "56,1,user,Can I return a product without a receipt?,2024-07-20 09:00:00\n",
      "\n",
      "56,2,bot,Returns without a receipt are subject to store policy and may only be exchanged for store credit.,2024-07-20 09:00:02\n",
      "\n",
      "57,1,user,Do you offer catering services?,2024-07-20 10:00:00\n",
      "\n",
      "57,2,bot,Yes, we offer catering services. Please contact customer service for more details.,2024-07-20 10:00:02\n",
      "\n",
      "58,1,user,Can I find kosher products in your store?,2024-07-20 11:00:00\n",
      "\n",
      "58,2,bot,Yes, we have a selection of kosher products.,2024-07-20 11:00:02\n",
      "\n",
      "59,1,user,Do you sell lottery tickets?,2024-07-20 12:00:00\n",
      "\n",
      "59,2,bot,Yes, we sell lottery tickets at the customer service desk.,2024-07-20 12:00:02\n",
      "\n",
      "60,1,user,Can I get help with a large order?,2024-07-20 13:00:00\n",
      "\n",
      "60,2,bot,Yes, our staff can assist you with large orders. Please let us know in advance.,2024-07-20 13:00:02\n",
      "\n",
      "61,1,user,What is the price of a rotisserie chicken?,2024-07-20 14:00:00\n",
      "\n",
      "61,2,bot,A rotisserie chicken is priced at $7.99.,2024-07-20 14:00:02\n",
      "\n",
      "62,1,user,Do you sell fresh herbs?,2024-07-20 15:00:00\n",
      "\n",
      "62,2,bot,Yes, we have a variety of fresh herbs available.,2024-07-20 15:00:02\n",
      "\n",
      "63,1,user,Can I use coupons at self-checkout?,2024-07-20 16:00:00\n",
      "\n",
      "63,2,bot,Yes, you can use coupons at self-checkout kiosks.,2024-07-20 16:00:02\n",
      "\n",
      "64,1,user,What is the price of strawberries?,2024-07-20 17:00:00\n",
      "\n",
      "64,2,bot,Strawberries are priced at $3.99 per pound.,2024-07-20 17:00:02\n",
      "\n",
      "65,1,user,Do you sell alcohol-free wine?,2024-07-20 18:00:00\n",
      "\n",
      "65,2,bot,Yes, we have alcohol-free wine options.,2024-07-20 18:00:02\n",
      "\n",
      "66,1,user,Can I get assistance with finding products?,2024-07-20 19:00:00\n",
      "\n",
      "66,2,bot,Yes, our staff can help you find products in the store.,2024-07-20 19:00:02\n",
      "\n",
      "67,1,user,Do you have a lost and found?,2024-07-20 20:00:00\n",
      "\n",
      "67,2,bot,Yes, our lost and found is located at the customer service desk.,2024-07-20 20:00:02\n",
      "\n",
      "68,1,user,What is the price of a whole turkey?,2024-07-20 21:00:00\n",
      "\n",
      "68,2,bot,A whole turkey is priced at $1.99 per pound.,2024-07-20 21:00:02\n",
      "\n",
      "69,1,user,Do you sell magazines?,2024-07-20 22:00:00\n",
      "\n",
      "69,2,bot,Yes, we have a variety of magazines available.,2024-07-20 22:00:02\n",
      "\n",
      "70,1,user,Can I get a discount for buying in bulk?,2024-07-21 09:00:00\n",
      "\n",
      "70,2,bot,Yes, we offer discounts on bulk purchases. Please check with our customer service for details.,2024-07-21 09:00:02\n",
      "\n",
      "71,1,user,Do you sell frozen meals?,2024-07-21 10:00:00\n",
      "\n",
      "71,2,bot,Yes, we have a wide range of frozen meals available.,2024-07-21 10:00:02\n",
      "\n",
      "72,1,user,Can I return a gift card?,2024-07-21 11:00:00\n",
      "\n",
      "72,2,bot,Gift cards are non-refundable.,2024-07-21 11:00:02\n",
      "\n",
      "73,1,user,Do you offer discounts for students?,2024-07-21 12:00:00\n",
      "\n",
      "73,2,bot,Yes, we offer a 5% discount for students with a valid student ID.,2024-07-21 12:00:02\n",
      "\n",
      "74,1,user,Can I buy fresh seafood?,2024-07-21 13:00:00\n",
      "\n",
      "74,2,bot,Yes, we have a fresh seafood section.,2024-07-21 13:00:02\n",
      "\n",
      "75,1,user,What is the price of oranges?,2024-07-21 14:00:00\n",
      "\n",
      "75,2,bot,Oranges are priced at $1.20 per pound.,2024-07-21 14:00:02\n",
      "\n",
      "76,1,user,Do you have a bakery that makes custom bread?,2024-07-21 15:00:00\n",
      "\n",
      "76,2,bot,Yes, our bakery can make custom bread to your specifications.,2024-07-21 15:00:02\n",
      "\n",
      "77,1,user,Can I buy a subscription to a meal kit service?,2024-07-21 16:00:00\n",
      "\n",
      "77,2,bot,Yes, we offer meal kit subscriptions. Please check our website for details.,2024-07-21 16:00:02\n",
      "\n",
      "78,1,user,Do you sell ice cream cakes?,2024-07-21 17:00:00\n",
      "\n",
      "78,2,bot,Yes, we have a selection of ice cream cakes available.,2024-07-21 17:00:02\n",
      "\n",
      "79,1,user,What is the price of blueberries?,2024-07-21 18:00:00\n",
      "\n",
      "79,2,bot,Blueberries are priced at $4.50 per pound.,2024-07-21 18:00:02\n",
      "\n",
      "80,1,user,Do you offer free samples?,2024-07-21 19:00:00\n",
      "\n",
      "80,2,bot,Yes, we offer free samples in various sections of the store.,2024-07-21 19:00:02\n",
      "\n",
      "81,1,user,Can I find organic snacks?,2024-07-21 20:00:00\n",
      "\n",
      "81,2,bot,Yes, we have a variety of organic snacks.,2024-07-21 20:00:02\n",
      "\n",
      "82,1,user,Do you sell household appliances?,2024-07-21 21:00:00\n",
      "\n",
      "82,2,bot,Yes, we have a selection of small household appliances.,2024-07-21 21:00:02\n",
      "\n",
      "83,1,user,What is the price of your rotisserie chicken?,2024-07-21 22:00:00\n",
      "\n",
      "83,2,bot,Our rotisserie chicken is priced at $7.99.,2024-07-21 22:00:02\n",
      "\n",
      "84,1,user,Can I buy a fruit basket?,2024-07-22 09:00:00\n",
      "\n",
      "84,2,bot,Yes, we offer pre-made and custom fruit baskets.,2024-07-22 09:00:02\n",
      "\n",
      "85,1,user,Do you have a section for organic produce?,2024-07-22 10:00:00\n",
      "\n",
      "85,2,bot,Yes, we have a dedicated section for organic produce.,2024-07-22 10:00:02\n",
      "\n",
      "86,1,user,Can I get a refund for an item bought on sale?,2024-07-22 11:00:00\n",
      "\n",
      "86,2,bot,Yes, you can get a refund for sale items with a receipt.,2024-07-22 11:00:02\n",
      "\n",
      "87,1,user,What is the price of lemons?,2024-07-22 12:00:00\n",
      "\n",
      "87,2,bot,Lemons are priced at $0.75 each.,2024-07-22 12:00:02\n",
      "\n",
      "88,1,user,Do you have a section for baby products?,2024-07-22 13:00:00\n",
      "\n",
      "88,2,bot,Yes, we have a wide range of baby products available.,2024-07-22 13:00:02\n",
      "\n",
      "89,1,user,Can I get a custom gift basket made?,2024-07-22 14:00:00\n",
      "\n",
      "89,2,bot,Yes, we can create custom gift baskets based on your preferences.,2024-07-22 14:00:02\n",
      "\n",
      "90,1,user,What is the price of organic honey?,2024-07-22 15:00:00\n",
      "\n",
      "90,2,bot,Organic honey is priced at $7.99 per jar.,2024-07-22 15:00:02\n",
      "\n",
      "91,1,user,Do you sell greeting cards?,2024-07-22 16:00:00\n",
      "\n",
      "91,2,bot,Yes, we have a selection of greeting cards for various occasions.,2024-07-22 16:00:02\n",
      "\n",
      "92,1,user,Can I get a discount for being a frequent shopper?,2024-07-22 17:00:00\n",
      "\n",
      "92,2,bot,Yes, our loyalty program offers discounts for frequent shoppers.,2024-07-22 17:00:02\n",
      "\n",
      "93,1,user,Do you offer flu shots?,2024-07-22 18:00:00\n",
      "\n",
      "93,2,bot,Yes, our pharmacy offers flu shots. No appointment is necessary.,2024-07-22 18:00:02\n",
      "\n",
      "94,1,user,What is the price of organic chicken breasts?,2024-07-22 19:00:00\n",
      "\n",
      "94,2,bot,Organic chicken breasts are priced at $6.99 per pound.,2024-07-22 19:00:02\n",
      "\n",
      "95,1,user,Do you have a section for eco-friendly products?,2024-07-22 20:00:00\n",
      "\n",
      "95,2,bot,Yes, we have a dedicated section for eco-friendly products.,2024-07-22 20:00:02\n",
      "\n",
      "96,1,user,Can I buy a gift card online?,2024-07-22 21:00:00\n",
      "\n",
      "96,2,bot,Yes, you can purchase gift cards online.,2024-07-22 21:00:02\n",
      "\n",
      "97,1,user,What is the price of gluten-free bread?,2024-07-22 22:00:00\n",
      "\n",
      "97,2,bot,Gluten-free bread is priced at $4.50 per loaf.,2024-07-22 22:00:02\n",
      "\n",
      "98,1,user,Do you sell frozen yogurt?,2024-07-23 09:00:00\n",
      "\n",
      "98,2,bot,Yes, we have a variety of frozen yogurt flavors available.,2024-07-23 09:00:02\n",
      "\n",
      "99,1,user,Can I get assistance with a heavy item?,2024-07-23 10:00:00\n",
      "\n",
      "99,2,bot,Yes, our staff can assist you with heavy items. Just ask for help at the customer service desk.,2024-07-23 10:00:02\n",
      "\n",
      "100,1,user,What is the price of your deli cheese?,2024-07-23 11:00:00\n",
      "\n",
      "100,2,bot,Prices for deli cheese range from $5.99 to $14.99 per pound depending on the type.,2024-07-23 11:00:02\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Team 1:\n",
      "\n",
      "Date | name | task name | update\n",
      "\n",
      "1-08-2024 Mohamed Faraazman Bin Farooq Chatbot E-D Lstm modified code- \n",
      "\n",
      "\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "from tensorflow.keras.models import Model\n",
      "\n",
      "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
      "\n",
      "from tensorflow.keras.preprocessing.text import Tokenizer\n",
      "\n",
      "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
      "\n",
      "from tensorflow.keras.utils import to_categorical\n",
      "\n",
      "\n",
      "\n",
      "# Load dataset\n",
      "\n",
      "df = pd.read_csv('your_dataset.csv')\n",
      "\n",
      "\n",
      "\n",
      "# Preprocess dataset\n",
      "\n",
      "conversations = []\n",
      "\n",
      "for i in range(0, len(df), 2):\n",
      "\n",
      "    conversations.append([df.iloc[i]['message'], df.iloc[i+1]['message']])\n",
      "\n",
      "\n",
      "\n",
      "input_texts = [conv[0] for conv in conversations]\n",
      "\n",
      "target_texts = ['\\t' + conv[1] + '\\n' for conv in conversations]\n",
      "\n",
      "\n",
      "\n",
      "# Tokenize the input and target texts\n",
      "\n",
      "tokenizer_input = Tokenizer()\n",
      "\n",
      "tokenizer_target = Tokenizer()\n",
      "\n",
      "tokenizer_input.fit_on_texts(input_texts)\n",
      "\n",
      "tokenizer_target.fit_on_texts(target_texts)\n",
      "\n",
      "\n",
      "\n",
      "input_sequences = tokenizer_input.texts_to_sequences(input_texts)\n",
      "\n",
      "target_sequences = tokenizer_target.texts_to_sequences(target_texts)\n",
      "\n",
      "\n",
      "\n",
      "max_encoder_seq_length = max([len(seq) for seq in input_sequences])\n",
      "\n",
      "max_decoder_seq_length = max([len(seq) for seq in target_sequences])\n",
      "\n",
      "\n",
      "\n",
      "input_data = pad_sequences(input_sequences, maxlen=max_encoder_seq_length, padding='post')\n",
      "\n",
      "target_data = pad_sequences(target_sequences, maxlen=max_decoder_seq_length, padding='post')\n",
      "\n",
      "\n",
      "\n",
      "num_encoder_tokens = len(tokenizer_input.word_index) + 1\n",
      "\n",
      "num_decoder_tokens = len(tokenizer_target.word_index) + 1\n",
      "\n",
      "\n",
      "\n",
      "encoder_input_data = to_categorical(input_data, num_encoder_tokens)\n",
      "\n",
      "decoder_input_data = to_categorical(target_data, num_decoder_tokens)\n",
      "\n",
      "\n",
      "\n",
      "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens))\n",
      "\n",
      "for i, seq in enumerate(target_sequences):\n",
      "\n",
      "    for t, token in enumerate(seq):\n",
      "\n",
      "        if t > 0:\n",
      "\n",
      "            decoder_target_data[i, t - 1, token] = 1.0\n",
      "\n",
      "\n",
      "\n",
      "# the model architecture\n",
      "\n",
      "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
      "\n",
      "encoder = LSTM(256, return_state=True)\n",
      "\n",
      "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
      "\n",
      "encoder_states = [state_h, state_c]\n",
      "\n",
      "\n",
      "\n",
      "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
      "\n",
      "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
      "\n",
      "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
      "\n",
      "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
      "\n",
      "decoder_outputs = decoder_dense(decoder_outputs)\n",
      "\n",
      "\n",
      "\n",
      "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
      "\n",
      "\n",
      "\n",
      "# Compile and train the model\n",
      "\n",
      "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
      "\n",
      "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
      "\n",
      "          batch_size=64, epochs=100, validation_split=0.2)\n",
      "\n",
      "\n",
      "\n",
      "# Defines the inference models\n",
      "\n",
      "encoder_model = Model(encoder_inputs, encoder_states)\n",
      "\n",
      "\n",
      "\n",
      "decoder_state_input_h = Input(shape=(256,))\n",
      "\n",
      "decoder_state_input_c = Input(shape=(256,))\n",
      "\n",
      "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
      "\n",
      "\n",
      "\n",
      "decoder_outputs, state_h, state_c = decoder_lstm(\n",
      "\n",
      "    decoder_inputs, initial_state=decoder_states_inputs)\n",
      "\n",
      "decoder_states = [state_h, state_c]\n",
      "\n",
      "decoder_outputs = decoder_dense(decoder_outputs)\n",
      "\n",
      "decoder_model = Model(\n",
      "\n",
      "    [decoder_inputs] + decoder_states_inputs,\n",
      "\n",
      "    [decoder_outputs] + decoder_states)\n",
      "\n",
      "\n",
      "\n",
      "def decode_sequence(input_seq):\n",
      "\n",
      "    states_value = encoder_model.predict(input_seq)\n",
      "\n",
      "\n",
      "\n",
      "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
      "\n",
      "    target_seq[0, 0, tokenizer_target.word_index['\\t']] = 1.0\n",
      "\n",
      "\n",
      "\n",
      "    stop_condition = False\n",
      "\n",
      "    decoded_sentence = ''\n",
      "\n",
      "    while not stop_condition:\n",
      "\n",
      "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
      "\n",
      "\n",
      "\n",
      "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
      "\n",
      "        sampled_char = tokenizer_target.index_word[sampled_token_index]\n",
      "\n",
      "        decoded_sentence += sampled_char\n",
      "\n",
      "\n",
      "\n",
      "        if sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length:\n",
      "\n",
      "            stop_condition = True\n",
      "\n",
      "\n",
      "\n",
      "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
      "\n",
      "        target_seq[0, 0, sampled_token_index] = 1.0\n",
      "\n",
      "\n",
      "\n",
      "        states_value = [h, c]\n",
      "\n",
      "\n",
      "\n",
      "    return decoded_sentence\n",
      "\n",
      "\n",
      "\n",
      "# Testing the model\n",
      "\n",
      "for seq_index in range(10):\n",
      "\n",
      "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
      "\n",
      "    decoded_sentence = decode_sequence(input_seq)\n",
      "\n",
      "    print('Input:', input_texts[seq_index])\n",
      "\n",
      "    print('Decoded:', decoded_sentence)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Team 2:\n",
      "\n",
      "Date | name | task name | update' metadata={'source': './docs/project-chatbot.docx'}\n"
     ]
    }
   ],
   "source": [
    "# Inspect the first document object\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GoABujz3IsHh"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "class CustomEmbedding:\n",
    "    def __init__(self, model_name):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        # Compute embeddings\n",
    "        return self.model.encode(texts, convert_to_tensor=True).tolist()\n",
    "\n",
    "# Initialize custom embedding\n",
    "embedding = CustomEmbedding('all-MiniLM-L6-v2')\n",
    "\n",
    "# Assume documents is a list of text strings\n",
    "texts = [doc for doc in documents]  # if documents is a list of text strings\n",
    "\n",
    "# Create and persist a vector store\n",
    "vectordb = Chroma.from_documents(\n",
    "    texts,\n",
    "    embedding=embedding\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2SHDWBYMKRJ5",
    "outputId": "f93fd667-ac28-47ac-84e1-b3b3b7c1ec46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-chroma in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
      "Requirement already satisfied: chromadb<0.6.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-chroma) (0.5.5)\n",
      "Requirement already satisfied: fastapi<1,>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from langchain-chroma) (0.112.0)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.40 in /usr/local/lib/python3.10/dist-packages (from langchain-chroma) (0.2.28)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-chroma) (1.26.4)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.2.1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (2.8.2)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.7.6)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.30.5)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.18.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.26.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.19.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (4.66.4)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (6.4.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.64.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (4.2.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.12.3)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (30.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (8.5.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (6.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (3.10.6)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.27.0)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi<1,>=0.95.2->langchain-chroma) (0.37.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.40->langchain-chroma) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.40->langchain-chroma) (0.1.98)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.40->langchain-chroma) (24.1)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.1.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb<0.6.0,>=0.4.0->langchain-chroma) (2.0.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (3.7.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.0.5)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (3.7)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.40->langchain-chroma) (3.0.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (2.27.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.8.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (2.31.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (2.0.7)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain-chroma) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain-chroma) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain-chroma) (3.20.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=8.0.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (8.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.63.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.26.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.26.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.47b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.47b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.47b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.47b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.47b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (71.0.4)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb<0.6.0,>=0.4.0->langchain-chroma) (2.20.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.23.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (13.7.1)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.22.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain-chroma) (12.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.2.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (5.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (4.9)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb<0.6.0,>=0.4.0->langchain-chroma) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb<0.6.0,>=0.4.0->langchain-chroma) (2024.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.0.0,>=6.0->opentelemetry-api>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (3.19.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (3.3.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (2.16.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain-chroma) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain-chroma) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain-chroma) (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain-chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rEhbXJQEKS2Y"
   },
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRyvER-zE0Ls"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"use your api man\"\n",
    "for file in os.listdir('docs'):\n",
    "    if file.endswith('.pdf'):\n",
    "        loader = PyPDFLoader('./docs/' + file)\n",
    "        documents.extend(loader.load())\n",
    "    elif file.endswith('.docx') or file.endswith('.doc'):\n",
    "        loader = Docx2txtLoader('./docs/' + file)\n",
    "        documents.extend(loader.load())\n",
    "    elif file.endswith('.txt'):\n",
    "        loader = TextLoader('./docs/' + file)\n",
    "        documents.extend(loader.load())\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunked_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    chunked_documents,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    persist_directory='./data'\n",
    ")\n",
    "vectordb.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UJQV9ZHLNkL7",
    "outputId": "e0fa55ff-571a-461c-f5a2-3544f5932874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: who are team mates\n",
      "Answer: Based on the provided context, the only individual mentioned is Mohamed Faraazman Bin Farooq. There is no explicit mention of other team members or teammates.\n",
      "Prompt: are there any vegan options available\n",
      "Answer: Yes, we have a variety of vegan options available in the store.\n",
      "Prompt: do they sell gift cards\n",
      "Answer: Yes, they sell gift cards in various denominations.\n",
      "Prompt: who is mohamed faraazman\n",
      "Answer: Mohamed Faraazman bin Farooq S is a fourth-year student majoring in Artificial Intelligence and Data Science at BSA Crescent University in Chennai. He has a CGPA of 9.1/10 and has a profound interest in the interdisciplinary applications of AI and data science. He is preparing for an internship at KAUST's Visiting Student Research Program and has a strong background in programming languages and tools like Python, Java, C, SQL, and various others. Mohamed has worked on projects related to personalized medicine and is particularly interested in projects involving networks or graphs, especially in biological research. He is eager to contribute to complex problems in these areas and is excited about the opportunity to work in a collaborative research environment.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    ChatOpenAI(),\n",
    "    vectordb.as_retriever(search_kwargs={'k': 7}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "chat_history = []\n",
    "while True:\n",
    "    query = input('Prompt: ')\n",
    "    if query.lower() in [\"exit\", \"quit\", \"q\"]:\n",
    "        print('Exiting')\n",
    "        break\n",
    "    result = qa_chain.invoke({'question': query, 'chat_history': chat_history})\n",
    "    print('Answer: ' + result['answer'])\n",
    "    chat_history.append((query, result['answer']))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
